# Tokenization

A data security process that replaces sensitive information with unique identification symbols (tokens) that retain essential information without exposing the original data.